# -*- coding: utf-8 -*-
"""2019.05_Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12VELN5z166rRQbGfInm3i0M5DSEjgNaf
"""

# Import statements

import pandas as pd
import numpy as np
import random

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from IPython.core.display import display, HTML

# import matplotlib.pyplot as plt

!wget -nc -P decision_tree/ https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv

# Read the data.
titan = pd.read_csv("decision_tree/titanic.csv")

"""
the following part is exploratory analysis
"""
titan.head()

# plot the histogram based on age only
age = np.array(titan["Age"])
hist = [go.Histogram(x = age)]
fig = go.Figure(hist)
fig.show()

# plot the histogram based on age and survival
age0 = np.array(titan[titan["Survived"] == 0]["Age"])
age1 = np.array(titan[titan["Survived"] == 1]["Age"])

trace1 = go.Histogram(
    x=age0,
    opacity=0.75
)
trace2 = go.Histogram(
    x=age1,
    opacity=0.75
)

data = [trace1, trace2]
layout = go.Layout(barmode='overlay')
fig = go.Figure(data=data, layout=layout)

fig.show()
# from the histogram above, we cannot really tell the difference between
# survived/deceased group

colors = [[0, "rgb(0,0,255)"], [0.1, "rgb(51,153,255)"], [0.2, "rgb(102,204,255)"], [0.3, "rgb(153,204,255)"], [0.4, "rgb(204,204,255)"], [0.5, "rgb(255,255,255)"], [0.6, "rgb(255,204,255)"], [0.7, "rgb(255,153,255)"], [0.8, "rgb(255,102,204)"], [0.9, "rgb(255,102,102)"], [1, "rgb(255,0,0)"]]

# some correlation plot
corr = np.array(titan.corr())
cols = list(titan.corr().columns)

fig = go.Figure(go.Heatmap(z = corr, x = cols, y = cols, colorscale=colors))
fig.show()
# correlation marix

"""
this part starts to predict
"""

titan2 = titan.drop(["Name"], axis = 1)
# convert Sex to categorical, and then integer
titan2.Sex = titan2.Sex.astype("category", ordered = True, \
    categories = titan2.Sex.unique()).cat.codes

# train test split
n = titan.shape[0]
order = random.sample(range(0,n),n)  # random shuffle
X = np.array(titan2.iloc[:,1:])
y = np.array(titan.iloc[:,0])

X_train = X[order[0:600],:]
X_test = X[order[600:],:]

y_train = y[order[0:600]]
y_test = y[order[600:]]

model = DecisionTreeClassifier()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

# Calculate the accuracy and assign it to the variable acc.
acc = accuracy_score(y_test, y_pred)

print(acc)

"""
adding cross-validation for better results
"""
# from sklearn.model_selection import GridSearchCV
parameters = {'max_depth':range(3,20)}
clf = GridSearchCV(DecisionTreeClassifier(),\
    parameters, n_jobs=8, cv = 10)
clf.fit(X=X_train, y=y_train)
tree_model = clf.best_estimator_
print (clf.best_score_, clf.best_params_)